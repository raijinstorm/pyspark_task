{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4885ce62",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "Использование JDBC для коннекта спарка и БД\n",
    "\n",
    "---\n",
    "Инкапсуляция импорта таблиц из БД в датафреймы с помощью функции *read_table_from_db*\n",
    "\n",
    "---\n",
    "Оптимизация запросов. \n",
    "1. Запросы я писал по аналогии моим запросам на SQL, по сути менялся только синтаксис. \n",
    "2. Использовал иногда broadcast, но только в случае, когда таблица заведомо небольшая. Вообще, в теории, тут много чего можно было в броадкаст засунуть т.к. в целом данных не много, но я решил этого не делать, чтобы при увелечении кол-ва данных +- все запросы все также работали.\n",
    "3. Также ничего не кешировал; в случае, если все 7 запросов выпольнялись бы последовательно, это могло бы дать прирост производительности, но я расценивал каждый таск как независимую единицу, а в таком случае кеширование ничего абсолютно не давало.\n",
    "4. Можно было бы еще для каждого запроса перед джоинами выбирать только нужные колонки, но на сколько я знаю Catalyst Optimizer достаточно умен, что сделать это самому пользуясь column pruning\n",
    "\n",
    "\n",
    "---\n",
    "spark.sql.shuffle.partitions = 50 \n",
    "\n",
    "Установил такое количество, пользуясь рекомендацией, что кол-во партиций должно быть в 2-3 раза больше количества ядер ( у меня 16 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663eae37",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f17678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/04 09:34:05 WARN Utils: Your hostname, athena resolves to a loopback address: 127.0.1.1; using 10.255.255.254 instead (on interface lo)\n",
      "25/07/04 09:34:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "25/07/04 09:34:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "jdbc_path = \"/home/hello/.jdbc/postgresql-42.7.7.jar\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PagilaInNotebook\") \\\n",
    "    .config(\"spark.jars\", jdbc_path) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "url = \"jdbc:postgresql://localhost:5432/pagila\"\n",
    "\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7eb50643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_table_from_db(table_name):\n",
    "    return spark.read.format(\"jdbc\") \\\n",
    "    .option(\"url\", url) \\\n",
    "    .option(\"dbtable\", table_name) \\\n",
    "    .options(**{\"user\": os.getenv(\"DB_USER\"), \"password\": os.getenv(\"PASSWORD\"), \"driver\": \"org.postgresql.Driver\"}) \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2898602",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79048c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------+\n",
      "|name       |num_of_films|\n",
      "+-----------+------------+\n",
      "|Drama      |152         |\n",
      "|Music      |152         |\n",
      "|Travel     |151         |\n",
      "|Foreign    |150         |\n",
      "|Children   |150         |\n",
      "|Games      |150         |\n",
      "|Sci-Fi     |149         |\n",
      "|Action     |149         |\n",
      "|Animation  |148         |\n",
      "|Family     |147         |\n",
      "|Classics   |147         |\n",
      "|New        |147         |\n",
      "|Sports     |145         |\n",
      "|Documentary|145         |\n",
      "|Comedy     |143         |\n",
      "|Horror     |142         |\n",
      "+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_category = read_table_from_db(\"public.category\")\n",
    "df_film_category = read_table_from_db(\"public.film_category\")\n",
    "\n",
    "result_df = df_film_category.join(broadcast(df_category), on=\"category_id\") \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .agg(count(\"name\").alias(\"num_of_films\")) \\\n",
    "    .orderBy(desc(col(\"num_of_films\")))\n",
    "\n",
    "result_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "450db939",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7270fced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+\n",
      "|        actor_name|rent_count|\n",
      "+------------------+----------+\n",
      "|       SUSAN DAVIS|       825|\n",
      "|    GINA DEGENERES|       753|\n",
      "|    MATTHEW CARREY|       678|\n",
      "|       MARY KEITEL|       674|\n",
      "|ANGELA WITHERSPOON|       654|\n",
      "|       WALTER TORN|       640|\n",
      "|       HENRY BERRY|       612|\n",
      "|       JAYNE NOLTE|       611|\n",
      "|        VAL BOLGER|       605|\n",
      "|     SANDRA KILMER|       604|\n",
      "+------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum  \n",
    "\n",
    "df_rental = read_table_from_db(\"rental\")\n",
    "df_inventory = read_table_from_db(\"inventory\")\n",
    "df_film_actor = read_table_from_db(\"film_actor\")\n",
    "df_actor = read_table_from_db(\"actor\")\n",
    "\n",
    "film_count = df_rental.join(df_inventory, on=\"inventory_id\") \\\n",
    "    .groupBy(\"film_id\") \\\n",
    "    .count()\n",
    "\n",
    "\n",
    "df_actor = df_actor.withColumn(\n",
    "    \"actor_name\",\n",
    "    concat_ws(\" \", \"first_name\", \"last_name\")\n",
    ")\n",
    "\n",
    "\n",
    "res = df_film_actor.join(film_count, on=\"film_id\", how=\"left\") \\\n",
    "    .join(df_actor, on=\"actor_id\") \\\n",
    "    .groupBy(\"actor_name\") \\\n",
    "    .agg(_sum(\"count\").alias(\"rent_count\")) \\\n",
    "    .orderBy(desc(col(\"rent_count\")))\n",
    "    \n",
    "res.show(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183747e1",
   "metadata": {},
   "source": [
    "# Task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ccbb66d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------------+\n",
      "|   name|category_revenue|\n",
      "+-------+----------------+\n",
      "|Foreign|        10507.67|\n",
      "+-------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rental = read_table_from_db(\"rental\")\n",
    "df_inventory = read_table_from_db(\"inventory\")\n",
    "df_category = read_table_from_db(\"category\")\n",
    "df_film_category = read_table_from_db(\"film_category\")\n",
    "df_payment = read_table_from_db(\"payment\")\n",
    "\n",
    "\n",
    "df_film_revenue = df_payment.join(df_rental , on=\"rental_id\") \\\n",
    "    .join(df_inventory, on=\"inventory_id\") \\\n",
    "    .groupBy(\"film_id\") \\\n",
    "    .agg(_sum(\"amount\").alias(\"spent_on_film\"))\n",
    "    \n",
    "res = df_film_category.join(df_film_revenue, on=\"film_id\") \\\n",
    "    .join(df_category, on=\"category_id\") \\\n",
    "    .groupBy(\"name\") \\\n",
    "    .agg(_sum(\"spent_on_film\").alias(\"category_revenue\")) \\\n",
    "    .orderBy(desc(col(\"category_revenue\")))\n",
    "    \n",
    "res.show(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982b4b87",
   "metadata": {},
   "source": [
    "# Task 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc70b930",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|film_id|               title|\n",
      "+-------+--------------------+\n",
      "|    148|      CHOCOLATE DUCK|\n",
      "|    669|       PEARL DESTINY|\n",
      "|    713|       RAINBOW SHOCK|\n",
      "|    221|DELIVERANCE MULHO...|\n",
      "|    495|    KENTUCKIAN GIANT|\n",
      "|    712|   RAIDERS ANTITRUST|\n",
      "|    801|       SISTER FREDDY|\n",
      "|    943|   VILLAIN DESPERATE|\n",
      "|    359|  GLADIATOR WESTWARD|\n",
      "|    108|       BUTCH PANTHER|\n",
      "|    950|        VOLUME HOUSE|\n",
      "|     87|   BOONDOCK BALLROOM|\n",
      "|    642|      ORDER BETRAYED|\n",
      "|    171|COMMANDMENTS EXPRESS|\n",
      "|     33|         APOLLO TEEN|\n",
      "|    874|        TADPOLE PARK|\n",
      "|    497|    KILL BROTHERHOOD|\n",
      "|     14|      ALICE FANTASIA|\n",
      "|    198|    CRYSTAL BREAKING|\n",
      "|    332|FRANKENSTEIN STRA...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "+-------+--------------------+\n",
      "|film_id|               title|\n",
      "+-------+--------------------+\n",
      "|    148|      CHOCOLATE DUCK|\n",
      "|    669|       PEARL DESTINY|\n",
      "|    713|       RAINBOW SHOCK|\n",
      "|    221|DELIVERANCE MULHO...|\n",
      "|    495|    KENTUCKIAN GIANT|\n",
      "|    712|   RAIDERS ANTITRUST|\n",
      "|    801|       SISTER FREDDY|\n",
      "|    943|   VILLAIN DESPERATE|\n",
      "|    359|  GLADIATOR WESTWARD|\n",
      "|    108|       BUTCH PANTHER|\n",
      "|    950|        VOLUME HOUSE|\n",
      "|     87|   BOONDOCK BALLROOM|\n",
      "|    642|      ORDER BETRAYED|\n",
      "|    171|COMMANDMENTS EXPRESS|\n",
      "|     33|         APOLLO TEEN|\n",
      "|    874|        TADPOLE PARK|\n",
      "|    497|    KILL BROTHERHOOD|\n",
      "|     14|      ALICE FANTASIA|\n",
      "|    198|    CRYSTAL BREAKING|\n",
      "|    332|FRANKENSTEIN STRA...|\n",
      "+-------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_film = read_table_from_db(\"film\")\n",
    "df_inventory = read_table_from_db(\"inventory\")\n",
    "\n",
    "#Analogy to SQL query\n",
    "res = df_film.join(df_inventory, on=\"film_id\", how=\"left\") \\\n",
    "    .filter(col(\"inventory_id\").isNull()).dropDuplicates([\"film_id\"]).select([\"film_id\", \"title\"])\n",
    "    \n",
    "res.show()\n",
    "\n",
    "#PySpark way using anitjoin\n",
    "df_inventory_id = df_inventory.select(\"film_id\").distinct()\n",
    "res = df_film.join(df_inventory_id, on=\"film_id\", how=\"left_anti\").select(\"film_id\", \"title\")\n",
    "\n",
    "res.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70003ba6",
   "metadata": {},
   "source": [
    "# Task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "70e20dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/04 09:44:16 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|  actor_name|\n",
      "+------------+\n",
      "|RICHARD PENN|\n",
      "|EWAN GOODING|\n",
      "|SIDNEY CROWE|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_category = read_table_from_db(\"category\").alias(\"category\")\n",
    "df_film_category = read_table_from_db(\"film_category\").alias(\"film_category\")\n",
    "df_film_actor = read_table_from_db(\"film_actor\").alias(\"film_actor\")\n",
    "df_actor = read_table_from_db(\"actor\").alias(\"actor\")\n",
    "\n",
    "df_actor = df_actor.withColumn(\n",
    "    \"actor_name\",\n",
    "    concat_ws(\" \", \"first_name\", \"last_name\")\n",
    ")\n",
    "\n",
    "df_selected_actors = df_film_actor.join(df_film_category, on=\"film_id\") \\\n",
    "    .join(df_category, on=\"category_id\") \\\n",
    "    .join(df_actor, on=\"actor_id\") \\\n",
    "    .filter(col(\"category.name\") == \"Children\") \\\n",
    "    .groupBy(\"actor_name\") \\\n",
    "    .agg(count(col(\"film_actor.film_id\")).alias(\"cnt\"))\n",
    "    \n",
    "windowSpec = Window.orderBy(desc(col(\"cnt\")))\n",
    "\n",
    "top_actors = df_selected_actors.withColumn(\"rank\", rank().over(windowSpec)).filter(col(\"rank\") <= 3).select(\"actor_name\")\n",
    "\n",
    "top_actors.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d24f70a",
   "metadata": {},
   "source": [
    "# Task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed06f798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------+------------+\n",
      "|            city|active_cnt|inactive_cnt|\n",
      "+----------------+----------+------------+\n",
      "|          Kamyin|         0|           1|\n",
      "|         Bat Yam|         0|           1|\n",
      "|       Najafabad|         0|           1|\n",
      "|         Wroclaw|         0|           1|\n",
      "|          Amroha|         0|           1|\n",
      "|Charlotte Amalie|         0|           1|\n",
      "|          Daxian|         0|           1|\n",
      "|       Pingxiang|         0|           1|\n",
      "|   Coatzacoalcos|         0|           1|\n",
      "|     Szkesfehrvr|         0|           1|\n",
      "|        Uluberia|         0|           1|\n",
      "|        Xiangfan|         0|           1|\n",
      "|          Ktahya|         0|           1|\n",
      "| Southend-on-Sea|         0|           1|\n",
      "|      Kumbakonam|         0|           1|\n",
      "|         Lincoln|         1|           0|\n",
      "|     Addis Abeba|         1|           0|\n",
      "|           Sucre|         1|           0|\n",
      "|     Qinhuangdao|         1|           0|\n",
      "|        Tambaram|         1|           0|\n",
      "+----------------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_city = read_table_from_db(\"city\").alias(\"city\")\n",
    "df_address = read_table_from_db(\"address\").alias(\"address\")\n",
    "df_customer = read_table_from_db(\"customer\").alias(\"customer\")\n",
    "\n",
    "res = df_customer.join(df_address, on=\"address_id\") \\\n",
    "    .join(df_city, on=\"city_id\") \\\n",
    "    .groupBy(col(\"city.city\")) \\\n",
    "    .agg(\n",
    "        _sum(col(\"customer.active\")).alias(\"active_cnt\"), \n",
    "        count(col(\"customer.active\")).alias(\"cnt\")) \\\n",
    "    .withColumn(\"inactive_cnt\", col(\"cnt\") - col(\"active_cnt\")) \\\n",
    "    .orderBy(desc(col(\"inactive_cnt\"))) \\\n",
    "    .drop(\"cnt\")\n",
    "    \n",
    "res.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b543f96b",
   "metadata": {},
   "source": [
    "# Task 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef29978f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+------------+------------------+\n",
      "|   city_group|top_category|    total_duration|\n",
      "+-------------+------------+------------------+\n",
      "|     A_cities|    Children| 24021.29999999999|\n",
      "|Dashed cities|       Drama|14556.033333333326|\n",
      "+-------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum as _sum \n",
    "\n",
    "df_rental_joined = df_rental.join(df_inventory, on=\"inventory_id\") \\\n",
    "    .join(df_customer, on=\"customer_id\") \\\n",
    "    .join(df_address, on=\"address_id\") \\\n",
    "    .join(df_city, on=\"city_id\") \\\n",
    "    .join(df_film_category, on=\"film_id\") \\\n",
    "    .join(df_category, on=\"category_id\") \\\n",
    "    .filter((col(\"city\").like(\"%-%\")) | (col(\"city\").like(\"A%\"))) \\\n",
    "    .withColumn(\"city_group\", \n",
    "                when(col(\"city\").like(\"%-%\"), lit(\"Dashed cities\")) \n",
    "                .when(col(\"city\").like(\"A%\"), lit(\"A_cities\"))\n",
    "    ) \\\n",
    "    .withColumn(\"duration\" , (unix_timestamp(\"return_date\") - unix_timestamp(\"rental_date\")) / 3600)\n",
    "    \n",
    "\n",
    "df_rental_summary = df_rental_joined \\\n",
    "    .groupBy(\"city_group\", \"category.name\").agg(\n",
    "        count(\"rental_id\"),\n",
    "        _sum(\"duration\").alias(\"total_duration\")\n",
    "    )\n",
    "\n",
    "\n",
    "windowSpec = Window.orderBy(desc(col(\"total_duration\"))).partitionBy(\"city_group\")\n",
    "top_categories_in_selected_countries = df_rental_summary.withColumn(\"rank\", row_number().over(windowSpec)) \\\n",
    "    .filter(col(\"rank\") == 1).select(col(\"city_group\"), col(\"name\").alias(\"top_category\"), col(\"total_duration\"))\n",
    " \n",
    "top_categories_in_selected_countries.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
